{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: ['maggie_simpson', 'charles_montgomery_burns', 'patty_bouvier', 'ralph_wiggum', 'chief_wiggum']\n",
      "Test: ['principal_skinner_34.jpg', 'apu_nahasapeemapetilon_28.jpg', 'abraham_grampa_simpson_41.jpg', 'principal_skinner_20.jpg', 'apu_nahasapeemapetilon_14.jpg']\n"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "\n",
    "#configure train dataset\n",
    "train_root_path = \"./the-simpsons-characters-dataset-TINY/simpsons_dataset\"\n",
    "character_directories = listdir(train_root_path)\n",
    "#character_directories.remove('.DS_Store')\n",
    "print(\"Train: {}\".format(character_directories[:5]))\n",
    "\n",
    "#configure test dataset\n",
    "test_root_path = \"./the-simpsons-characters-dataset-TINY/kaggle_simpson_testset\"\n",
    "test_image_names = listdir(test_root_path)\n",
    "test_image_names.remove('.DS_Store')\n",
    "print(\"Test: {}\".format(test_image_names[:5]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate mean width and lenght from test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Width mean: 38\n",
      "Lenght mean: 37\n",
      "Size mean dimension: 37\n"
     ]
    }
   ],
   "source": [
    "import os, random\n",
    "from scipy.misc import imread, imresize\n",
    "\n",
    "width = 0\n",
    "lenght = 0\n",
    "num_test_images = len(test_image_names)\n",
    "\n",
    "for i in range(num_test_images):\n",
    "    path_file = os.path.join(test_root_path, test_image_names[i])\n",
    "    image = imread(path_file)\n",
    "    width += image.shape[0]\n",
    "    lenght += image.shape[1]\n",
    "\n",
    "width_mean = width//num_test_images\n",
    "lenght_mean = lenght//num_test_images\n",
    "dim_size = (width_mean + lenght_mean) // 2\n",
    "\n",
    "print(\"Width mean: {}\".format(width_mean))\n",
    "print(\"Lenght mean: {}\".format(lenght_mean))\n",
    "print(\"Size mean dimension: {}\".format(dim_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Size mean dimension will be used for the resizing process. __All the images will be scaled__ to __(37,37)__ since it's the average of the test images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show some test examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label:nelson_muntz, Image:400, Shape:(39, 26, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAJIAAADFCAYAAAC/8KvBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\neOG/AZopWf897LlqAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x117f6b2b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "idx = random.randint(0, num_test_images)\n",
    "sample_file, sample_name = test_image_names[idx], test_image_names[idx].split('_')[:-1]\n",
    "path_file = os.path.join(test_root_path, sample_file)\n",
    "sample_image = imread(path_file)\n",
    "\n",
    "print(\"Label:{}, Image:{}, Shape:{}\".format('_'.join(sample_name), idx, sample_image.shape))\n",
    "plt.figure(figsize=(3,3))\n",
    "plt.imshow(sample_image)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making batches (resized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_num_of_samples():\n",
    "    count = 0\n",
    "    for _,character in enumerate(character_directories):\n",
    "        path = os.path.join(train_root_path, character)\n",
    "        count += len(listdir(path))\n",
    "        \n",
    "    return count\n",
    "\n",
    "def get_batch(batch_init, batch_size):\n",
    "    data = {'image':[], 'label':[]}\n",
    "    character_batch_size = batch_size//len(character_directories)\n",
    "    character_batch_init = batch_init//len(character_directories)\n",
    "    character_batch_end = character_batch_init + character_batch_size\n",
    "    \n",
    "    for _,character in enumerate(character_directories):\n",
    "        path = os.path.join(train_root_path, character)\n",
    "        images_list = listdir(path)\n",
    "        for i in range(character_batch_init, character_batch_end):\n",
    "            if len(images_list) == 0:\n",
    "                continue\n",
    "            #if this character has small number of features\n",
    "            #we repeat them\n",
    "            if i >= len(images_list):\n",
    "                p = i % len(images_list)\n",
    "            else:\n",
    "                p = i\n",
    "                \n",
    "            path_file = os.path.join(path, images_list[p])\n",
    "            image = imread(path_file)\n",
    "            #all with the same shape\n",
    "            image = imresize(image, (dim_size, dim_size))\n",
    "            data['image'].append(image)\n",
    "            data['label'].append(character)\n",
    "    \n",
    "    return data\n",
    "\n",
    "def get_batches(num_batches, batch_size, verbose=False):\n",
    "    #num max of samples\n",
    "    num_samples = get_num_of_samples()\n",
    "    #check number of batches with the maximum\n",
    "    max_num_batches = num_samples//batch_size - 1\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Number of samples:{}\".format(num_samples))\n",
    "        print(\"Batches:{} Size:{}\".format(num_batches, batch_size))\n",
    "    assert num_batches <= max_num_batches, \"Surpassed the maximum number of batches\"\n",
    "        \n",
    "    for i in range(0, num_batches):\n",
    "        init = i * batch_size\n",
    "        if verbose:\n",
    "            print(\"Batch-{} yielding images from {} to {}...\".format(i, init, init+batch_size))\n",
    "        \n",
    "        yield get_batch(init, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples:20933\n",
      "Batches:10 Size:500\n",
      "Batch-0 yielding images from 0 to 500...\n",
      "\t|- retrieved 420 images\n",
      "Batch-1 yielding images from 500 to 1000...\n",
      "\t|- retrieved 420 images\n",
      "Batch-2 yielding images from 1000 to 1500...\n",
      "\t|- retrieved 420 images\n",
      "Batch-3 yielding images from 1500 to 2000...\n",
      "\t|- retrieved 420 images\n",
      "Batch-4 yielding images from 2000 to 2500...\n",
      "\t|- retrieved 420 images\n",
      "Batch-5 yielding images from 2500 to 3000...\n",
      "\t|- retrieved 420 images\n",
      "Batch-6 yielding images from 3000 to 3500...\n",
      "\t|- retrieved 420 images\n",
      "Batch-7 yielding images from 3500 to 4000...\n",
      "\t|- retrieved 420 images\n",
      "Batch-8 yielding images from 4000 to 4500...\n",
      "\t|- retrieved 420 images\n",
      "Batch-9 yielding images from 4500 to 5000...\n",
      "\t|- retrieved 420 images\n"
     ]
    }
   ],
   "source": [
    "#testing generator\n",
    "batch_size = 500\n",
    "\n",
    "for b in get_batches(10, batch_size, verbose=True):\n",
    "    print(\"\\t|- retrieved {} images\".format(len(b['image'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "#num characters\n",
    "num_characters = len(character_directories)\n",
    "\n",
    "#normalize\n",
    "def normalize(x):\n",
    "    #we use the feature scaling to have all the batches\n",
    "    #in the same space, that is (0,1)\n",
    "    return (x - np.amin(x))/(np.amax(x) - np.amin(x))\n",
    "\n",
    "#one-hot encode\n",
    "lb = preprocessing.LabelBinarizer()\n",
    "lb = lb.fit(character_directories)\n",
    "\n",
    "def one_hot(label):\n",
    "    return lb.transform([label])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing preprocessed batches on disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_batches = 40\n",
    "batch_size = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 16800 train images and stored on disk\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "cnt_images = 0\n",
    "for cnt, b in enumerate(get_batches(num_batches, batch_size)):\n",
    "    data = {'image':[], 'label':[]}\n",
    "    \n",
    "    for i in range( min(len(b['image']), batch_size) ):\n",
    "        image = np.array( b['image'][i] )\n",
    "        label = np.array( b['label'][i] )\n",
    "        #label = label.reshape([-1,:])\n",
    "        if len(image.shape) == 3:\n",
    "            data['image'].append(normalize(image))\n",
    "            data['label'].append(one_hot(label)[-1,:])\n",
    "            cnt_images += 1\n",
    "        else:\n",
    "            print(\"Dim image < 3\")\n",
    "    \n",
    "    with open(\"simpson_train_{}.pkl\".format(cnt), 'wb') as file:\n",
    "        pickle.dump(data, file, pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "print(\"Loaded {} train images and stored on disk\".format(cnt_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of onehot encoded:\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "#testing load from file\n",
    "import pickle\n",
    "\n",
    "with open('simpson_train_0.pkl', 'rb') as file:\n",
    "    data = pickle.load(file)\n",
    "    print(\"Example of onehot encoded:\\n{}\".format(data['label'][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "#helpers\n",
    "def convolution(inputs, kernel_shape, stride_shape, output_depth):\n",
    "    #convolution variables\n",
    "    input_depth = inputs.get_shape().as_list()[3]\n",
    "    filter_shape = kernel_shape + (input_depth, output_depth)\n",
    "    dev = 1/np.sqrt(kernel_shape[0]*kernel_shape[1])\n",
    "    filter_ = tf.Variable(tf.truncated_normal(filter_shape, stddev=dev), name=\"filter_\")\n",
    "    stride_shape = (1,) + stride_shape + (1,)\n",
    "    pool_shape = stride_shape\n",
    "    bias_ = tf.Variable(tf.truncated_normal([output_depth], stddev=dev), name=\"bias_\")\n",
    "    \n",
    "    #convolution\n",
    "    conv = tf.nn.conv2d(inputs, filter_, stride_shape, padding='SAME')\n",
    "    conv = tf.nn.bias_add(conv, bias_)\n",
    "    conv = tf.nn.relu(conv)\n",
    "    conv = tf.nn.conv2d(inputs, filter_, stride_shape, padding='SAME')\n",
    "    conv = tf.nn.bias_add(conv, bias_)\n",
    "    conv = tf.nn.relu(conv)\n",
    "    \n",
    "    #pooling\n",
    "    conv = tf.nn.max_pool(conv, pool_shape, stride_shape, padding='SAME')\n",
    "    \n",
    "    return conv\n",
    "\n",
    "def classifier(inputs, num_outputs):\n",
    "    #classifier variables\n",
    "    num_inputs = inputs.get_shape().as_list()[1]\n",
    "    dev = 1/np.sqrt(num_inputs)\n",
    "    weights = tf.Variable(tf.truncated_normal((num_inputs,)+num_outputs, stddev=dev), name=\"weights\")\n",
    "    bias = tf.Variable(tf.truncated_normal(num_outputs, stddev=dev), name=\"bias\")\n",
    "\n",
    "    #classifier\n",
    "    logits = tf.add(tf.matmul(inputs, weights), bias)\n",
    "\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, 37, 37, 3]\n",
      "[None, 10, 10, 32]\n",
      "[None, 3, 3, 64]\n",
      "Inputs shape: [37, 37, 3]\n",
      "Flatten shape: 576\n",
      "Outputs shape: 47\n"
     ]
    }
   ],
   "source": [
    "##building the network\n",
    "import numpy as np\n",
    "\n",
    "#remove previous weights, bias, etc\n",
    "tf.reset_default_graph()\n",
    "\n",
    "#shape\n",
    "image_shape = (dim_size, dim_size, 3)\n",
    "label_shape = (num_characters,)\n",
    "\n",
    "#data\n",
    "X = tf.placeholder(tf.float32, (None,) + image_shape)\n",
    "y = tf.placeholder(tf.float32, (None,) + label_shape)\n",
    "\n",
    "#conv\n",
    "print(X.get_shape().as_list())\n",
    "conv = convolution(X, (5,5), (2,2), 32)\n",
    "print(conv.get_shape().as_list())\n",
    "conv = convolution(X, (5,5), (2,2), 64)\n",
    "print(conv.get_shape().as_list())\n",
    "conv = convolution(conv, (5,5), (2,2), 128)\n",
    "print(conv.get_shape().as_list())\n",
    "    \n",
    "#before classifier\n",
    "flatten_shape = np.prod(conv.get_shape().as_list()[1:])\n",
    "flatten = tf.reshape(conv, [-1,flatten_shape])\n",
    "\n",
    "#classifying\n",
    "num_outputs = label_shape\n",
    "logits = classifier(flatten, (40,))\n",
    "logits = tf.nn.dropout(logits, 0.5)\n",
    "logits = classifier(logits, num_outputs)\n",
    "\n",
    "print(\"Inputs shape: {}\".format(X.get_shape().as_list()[1:]))\n",
    "print(\"Flatten shape: {}\".format(flatten_shape))\n",
    "print(\"Outputs shape: {}\".format(logits.get_shape().as_list()[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#loss and optmizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "#accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 Training Loss:3.5544 Validation Loss:3.7647 Accuracy:0.0833\n",
      "Epoch:1 Training Loss:3.0862 Validation Loss:3.6080 Accuracy:0.0952\n",
      "Epoch:2 Training Loss:2.7151 Validation Loss:3.1257 Accuracy:0.1786\n",
      "Epoch:3 Training Loss:2.4756 Validation Loss:2.9348 Accuracy:0.2381\n",
      "Epoch:4 Training Loss:2.0919 Validation Loss:2.7394 Accuracy:0.2619\n",
      "Epoch:5 Training Loss:1.9848 Validation Loss:2.6220 Accuracy:0.3095\n",
      "Epoch:6 Training Loss:1.8537 Validation Loss:2.4041 Accuracy:0.3452\n",
      "Epoch:7 Training Loss:1.7635 Validation Loss:2.4307 Accuracy:0.3571\n",
      "Epoch:8 Training Loss:1.5562 Validation Loss:2.2914 Accuracy:0.4048\n",
      "Epoch:9 Training Loss:1.6279 Validation Loss:2.2912 Accuracy:0.3571\n",
      "Epoch:10 Training Loss:1.5215 Validation Loss:2.1226 Accuracy:0.4643\n",
      "Epoch:11 Training Loss:1.4490 Validation Loss:2.0475 Accuracy:0.4762\n",
      "Epoch:12 Training Loss:1.3809 Validation Loss:2.1087 Accuracy:0.4524\n",
      "Epoch:13 Training Loss:1.3046 Validation Loss:1.9038 Accuracy:0.5238\n",
      "Epoch:14 Training Loss:1.2910 Validation Loss:1.9291 Accuracy:0.5000\n",
      "Epoch:15 Training Loss:1.2451 Validation Loss:1.6068 Accuracy:0.5476\n",
      "Epoch:16 Training Loss:1.1585 Validation Loss:1.7713 Accuracy:0.5476\n",
      "Epoch:17 Training Loss:1.1443 Validation Loss:1.6979 Accuracy:0.5714\n",
      "Epoch:18 Training Loss:1.0892 Validation Loss:1.7021 Accuracy:0.5357\n",
      "Epoch:19 Training Loss:1.1031 Validation Loss:1.8316 Accuracy:0.5595\n",
      "Epoch:20 Training Loss:0.9974 Validation Loss:1.6777 Accuracy:0.5595\n",
      "Epoch:21 Training Loss:0.9298 Validation Loss:1.5705 Accuracy:0.5476\n",
      "Epoch:22 Training Loss:1.0567 Validation Loss:1.7766 Accuracy:0.5595\n",
      "Epoch:23 Training Loss:0.9729 Validation Loss:1.7121 Accuracy:0.6310\n",
      "Epoch:24 Training Loss:0.9356 Validation Loss:1.6958 Accuracy:0.6071\n",
      "Epoch:25 Training Loss:0.8772 Validation Loss:1.8382 Accuracy:0.5833\n",
      "Epoch:26 Training Loss:0.8241 Validation Loss:1.7236 Accuracy:0.5833\n",
      "Epoch:27 Training Loss:0.8322 Validation Loss:1.2940 Accuracy:0.6310\n",
      "Epoch:28 Training Loss:0.8514 Validation Loss:1.6087 Accuracy:0.6190\n",
      "Epoch:29 Training Loss:0.8384 Validation Loss:1.6081 Accuracy:0.5952\n",
      "Epoch:30 Training Loss:0.8083 Validation Loss:1.6096 Accuracy:0.6548\n",
      "Epoch:31 Training Loss:0.8245 Validation Loss:1.4971 Accuracy:0.7024\n",
      "Epoch:32 Training Loss:0.7864 Validation Loss:1.3621 Accuracy:0.6667\n",
      "Epoch:33 Training Loss:0.7081 Validation Loss:1.5908 Accuracy:0.6548\n",
      "Epoch:34 Training Loss:0.7240 Validation Loss:1.4391 Accuracy:0.6786\n",
      "Epoch:35 Training Loss:0.7604 Validation Loss:1.5035 Accuracy:0.6905\n",
      "Epoch:36 Training Loss:0.7113 Validation Loss:1.5895 Accuracy:0.6190\n",
      "Epoch:37 Training Loss:0.6722 Validation Loss:1.3557 Accuracy:0.6667\n",
      "Epoch:38 Training Loss:0.6619 Validation Loss:1.4495 Accuracy:0.6548\n",
      "Epoch:39 Training Loss:0.6612 Validation Loss:1.4997 Accuracy:0.5714\n",
      "Epoch:40 Training Loss:0.5936 Validation Loss:1.5606 Accuracy:0.6548\n",
      "Epoch:41 Training Loss:0.6560 Validation Loss:1.6313 Accuracy:0.6667\n",
      "Epoch:42 Training Loss:0.6109 Validation Loss:1.4327 Accuracy:0.6667\n",
      "Epoch:43 Training Loss:0.5351 Validation Loss:1.4921 Accuracy:0.6310\n",
      "Epoch:44 Training Loss:0.6197 Validation Loss:1.6655 Accuracy:0.6310\n",
      "Epoch:45 Training Loss:0.6052 Validation Loss:1.5515 Accuracy:0.6786\n",
      "Epoch:46 Training Loss:0.5597 Validation Loss:1.4548 Accuracy:0.6667\n",
      "Epoch:47 Training Loss:0.5758 Validation Loss:1.5498 Accuracy:0.6786\n",
      "Epoch:48 Training Loss:0.4904 Validation Loss:1.7731 Accuracy:0.6429\n",
      "Epoch:49 Training Loss:0.5270 Validation Loss:1.5388 Accuracy:0.6786\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "##Train the model\n",
    "x_train = []\n",
    "y_train = []\n",
    "x_val = []\n",
    "y_val = []\n",
    "\n",
    "##\n",
    "epochs = 50\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for e in range(epochs):\n",
    "    \n",
    "    for i in range(num_batches):\n",
    "        fname = \"simpson_train_{}.pkl\".format(i)\n",
    "        if os.path.exists(fname):   \n",
    "            with open(fname, 'rb') as file:\n",
    "                #print(\"Processing: {}\".format(fname))\n",
    "                data = pickle.load(file)\n",
    "                x_train, x_val, y_train, y_val = train_test_split(data['image'], data['label'], test_size=0.2, random_state=42)\n",
    "                feed_dict = {X: x_train, y: y_train}\n",
    "                train_loss, _ = sess.run([cost, optimizer], feed_dict)\n",
    "\n",
    "                feed_dict = {X: x_val, y: y_val}\n",
    "                val_loss, acc = sess.run([cost, accuracy], feed_dict)\n",
    "            \n",
    "    print(\"Epoch:{} Training Loss:{:.4f} Validation Loss:{:.4f} Accuracy:{:.4f}\".format(e, train_loss, val_loss, acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: Bart Simpson\n",
      "Prediction: Bart Simpson\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAM0AAAChCAYAAABkijtIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11cee88d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#prediction\n",
    "idx = random.randint(0, num_test_images)\n",
    "sample_file, sample_name = test_image_names[idx], test_image_names[idx].split('_')[:-1]\n",
    "path_file = os.path.join(test_root_path, sample_file)\n",
    "sample_image = imread(path_file)\n",
    "idx = int(random.uniform(0, len(y_val)))\n",
    "    \n",
    "test_image = sample_image\n",
    "test_label = ' '.join([s.capitalize() for s in sample_name])\n",
    "\n",
    "test_image_norm = normalize(imresize(sample_image, (dim_size, dim_size)))\n",
    "\n",
    "prediction = sess.run(logits, {X:[test_image_norm]})\n",
    "prediction = lb.inverse_transform(prediction)\n",
    "\n",
    "#showing\n",
    "print(\"Label: {}\".format(test_label))\n",
    "prediction = ' '.join([s.capitalize() for s in prediction[0].split('_')])\n",
    "print(\"Prediction: {}\".format(prediction))\n",
    "\n",
    "plt.figure(figsize=(3,3))\n",
    "plt.imshow(test_image)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
